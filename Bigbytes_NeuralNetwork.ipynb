{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=navy>Import necessary libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import Sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix,accuracy_score, fbeta_score\n",
    "\n",
    "# Import Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#Import Keras API\n",
    "from keras.models import Sequential, load_model       \n",
    "from keras.layers import Dense, Activation\n",
    "import keras.backend as K\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import History "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=navy>Data Preparation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset and check distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.environ['DSX_PROJECT_DIR']+'/datasets/creditcard-training set v2.csv')\n",
    "df = df.rename(columns={ df.columns[2]: \"Fraud\" })\n",
    "\n",
    "#check the distribution\n",
    "# for row in df.head():\n",
    "#     fig=plt.figure(figsize=(17,10))\n",
    "#     df.hist(column=row)\n",
    "#     plt.xlabel(row,fontsize=15)\n",
    "#     plt.ylabel(\"Frequency\",fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unwanted rows and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows which has null values\n",
    "df = df.dropna()\n",
    "\n",
    "#drop feature 23 since it is meaningless in our analysis\n",
    "df = df.drop(['Feature 23'], axis=1)\n",
    "\n",
    "# Splitting our X and Y variables\n",
    "Y = df.Fraud\n",
    "Y = Y.values.reshape(Y.shape[0],1)\n",
    "\n",
    "df = df.drop('Fraud', 1)\n",
    "X = df.values\n",
    "\n",
    "#Normalize our data\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing set, oversampling of the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oversample(X_train,Y_train,print_output=False):\n",
    "    \n",
    "    #Convert back to dataframe for random oversampling\n",
    "    Train_set = np.concatenate((X_train, Y_train), axis=1)\n",
    "    df = pd.DataFrame.from_records(Train_set)\n",
    "\n",
    "    # Class count\n",
    "    count_class_0, count_class_1 = df.iloc[:,30].value_counts()\n",
    "\n",
    "    # Divide by class\n",
    "    df_class_0 = df[df.iloc[:,30] == 0]\n",
    "    df_class_1 = df[df.iloc[:,30] == 1]\n",
    "    \n",
    "    #Oversample the minority class and joining it back to original dataframe\n",
    "    df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "    df = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "    #Shuffle rows\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    #Splitting data back into X and Y variables\n",
    "    Y_train = df.iloc[:,30].values\n",
    "    X_train = df.iloc[:,0:30].values\n",
    "    \n",
    "    if print_output == True:\n",
    "        \n",
    "        #Print number of rows by class\n",
    "        print('Random over-sampling:')\n",
    "        print(df.iloc[:,30].value_counts())\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "#Split data into train and test set (80 to 20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "#Oversampling of our training set\n",
    "X_train, Y_train = Oversample(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=navy>Building and Training the Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Using Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a NN with 3 hidden layers using relu activation functions. \n",
    "#Output layer will use sigmoid function\n",
    "model = Sequential()\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with appropriate Loss function\n",
    "model.compile(optimizer = adam(), \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the model's output from probability into classes 1 or 0\n",
    "def Convert_prob_to_class(Y_test_hat):\n",
    "    \n",
    "    Y_test_hat[Y_test_hat > 0.5] = 1\n",
    "    Y_test_hat[Y_test_hat < 0.5] = 0\n",
    "\n",
    "#Plot the ROC graph\n",
    "def PlotROC(y_test,pred):\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "#Plot the loss history graph\n",
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#Printing the metrics and graphs of the model\n",
    "def PrintStats(y_test, pred, history, output_print=False):\n",
    "    \n",
    "    f1Score = round(f1_score(y_test, pred), 2)\n",
    "    recallScore = round(recall_score(y_test, pred), 2)\n",
    "    precscore = round(precision_score(y_test, pred), 2)\n",
    "    accScore = round(accuracy_score(y_test, pred), 2)\n",
    "    beta = 4\n",
    "    fbetaScore = round((1+beta) * (precscore * recallScore) / ((beta * precscore) + recallScore),2)\n",
    "    \n",
    "    #plot loss curve\n",
    "    if history is not None:\n",
    "        plot_history(history)\n",
    "    \n",
    "    #roc curve\n",
    "    auc = PlotROC(y_test,pred)\n",
    "    \n",
    "    print(\"Accuracy for Model : {acc_score}\".format(acc_score = accScore))\n",
    "    print(\"Precision for Model : {prec_score}\".format(prec_score = precscore))\n",
    "    print(\"Sensitivity/Recall for Model : {recall_score}\".format(recall_score = recallScore))\n",
    "    print(\"F1 Score for Model : {f1_score}\".format(f1_score = f1Score))\n",
    "    print(\"F-Beta Score for Model : {fbeta_score}\".format(fbeta_score = fbetaScore))\n",
    "    \n",
    "    output = [accScore,precscore,recallScore,f1Score, fbetaScore,auc]\n",
    "    \n",
    "    if output_print:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model (Alternatively, jump to loading of trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363754 samples, validate on 45553 samples\n",
      "Epoch 1/12\n",
      "  2232/363754 [..............................] - ETA: 4:27 - loss: 0.2105 - acc: 0.9140"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "epochs = 12\n",
    "batch_size = 72\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    validation_data = (X_test,Y_test), \n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained model into HDF5 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Saved_Neuralnetwork'\n",
    "\n",
    "model.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the trained model from HDF5 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Saved_Neuralnetwork'\n",
    "\n",
    "model = load_model(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the data on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run forward prop to get predicted values\n",
    "Y_test_hat = model.predict(X_test)\n",
    "Convert_prob_to_class(Y_test_hat)\n",
    "\n",
    "#If model is loaded from file, we will not have any history logs\n",
    "try:\n",
    "    history\n",
    "except NameError:\n",
    "    history = None\n",
    "\n",
    "#Print metrics and graphs\n",
    "PrintStats(Y_test,Y_test_hat,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=navy>Cross Validating the Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters for k-fold cross val\n",
    "n_folds = 6\n",
    "kf = KFold(n_folds)\n",
    "Metric_array = np.zeros(6)\n",
    "\n",
    "epochs = 12\n",
    "batch_size = 72\n",
    "counter = 1\n",
    "\n",
    "#Start k-fold cross validation \n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    #Split into training and testing set\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    #Oversampling of training set ONLY\n",
    "    X_train, Y_train = Oversample(X_train,Y_train)\n",
    "    \n",
    "    #run the model\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    #Run forward prop to get predicted values\n",
    "    Y_test_hat = model.predict(X_test)\n",
    "    Convert_prob_to_class(Y_test_hat)\n",
    "    \n",
    "    print('\\nMetrics for fold number: ' + str(counter))\n",
    "    counter += 1\n",
    "    \n",
    "    #Print metrics and store in numpy array to average after end of cv \n",
    "    Metric_array += PrintStats(Y_test,Y_test_hat,history,True)\n",
    "    \n",
    "#Averaging of metrics across folds    \n",
    "Metric_array /= n_folds\n",
    "\n",
    "print('\\nSummary metrics:')\n",
    "print(\"Accuracy for Model : {acc_score}\".format(acc_score = np.around(Metric_array[0],decimals = 2)))\n",
    "print(\"Precision for Model : {prec_score}\".format(prec_score = np.around(Metric_array[1],decimals = 2)))\n",
    "print(\"Sensitivity/Recall for Model : {recall_score}\".format(recall_score = np.around(Metric_array[2],decimals = 2)))\n",
    "print(\"F1 Score for Model : {f1_score}\".format(f1_score = np.around(Metric_array[3],decimals = 2)))\n",
    "print(\"F-Beta Score for Model : {f1_score}\".format(f1_score = np.around(Metric_array[4],decimals = 2)))\n",
    "print(\"AUC for Model : {auc}\".format(auc = Metric_array[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
